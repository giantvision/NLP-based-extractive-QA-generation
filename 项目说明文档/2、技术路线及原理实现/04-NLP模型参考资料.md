# NLP模型参考资料



## NEZHA: Neural Contextualized Representation for Chinese Language Understanding

- NEZHA：用于汉语理解的神经网络语境化表征

  论文地址：https://arxiv.org/abs/1909.00204

摘要：

预训练的语言模型在各种自然语言理解（NLU）任务中取得了巨大的成功，因为它能够通过对大规模语料库进行预训练来捕获文本中的深层上下文信息。在这份技术报告中，我们展示了我们在中文语料库上预训练名为 NEZHA（用于中文语言理解的神经上下文表示）的语言模型以及针对中文 NLU 任务进行微调的实践。当前版本的 Nezha 基于 BERT 并进行了一系列经过验证的改进，其中包括作为有效位置编码方案的功能相对位置编码、全字掩码策略、混合精度训练和模型训练中的 LAMB 优化器。

1、介绍

在这份技术报告中，我们介绍了我们对语言模型NEZHA（NEural contextualiZed representation for CHinese lAnguage understanding）进行预训练的做法，该模型目前是基于BERT并在中文文本上进行训练。具体来说，我们在模型中采用了一种叫做功能相对位置编码的技术。在vanilla Transformer以及BERT模型中，序列中每个词的位置编码是一个带有其绝对位置信息编码的向量。位置编码被添加到单词嵌入中，作为转化器的输入。有两种典型的方法来确定位置编码。一种是函数式位置编码，位置编码由一些预定义的函数（例如，[9]中的正弦函数）决定。另一种是参数性位置编码，它是模型参数的一部分，如[1]中的学习。[11]提出了一个参数化的相对位置编码，其中相对位置信息被纳入Transformer的自我注意层中。后来，Transformer-XL[12]和XLNet[6]提出使用一个正弦波编码矩阵和两个可训练的偏置项来表示相对位置。在这份技术报告中，我们采用了一种功能性的相对位置编码方案，它通过预先定义的函数来编码自我注意中的相对位置，没有任何可训练参数。我们的实证研究表明，对于预训练的语言模型来说，这是一个有效的位置编码方案，并且在我们的实验中取得了一致的收益。此外，我们在训练NEZHA时采用了三种在BERT模型的预训练中显示有效的技术，即整词屏蔽[8]、混合精度训练[13]和LAMB优化器[14]。

这份技术报告的贡献在于，我们系统地研究了在大规模中文语料库上预训练语言模型的问题，在几个中文 NLU 任务上评估模型，评估训练因素的有效性，包括位置编码方案、掩蔽策略、训练语料库的来源，训练序列的长度。我们将向社区发布我们的 Nezha 模型以及源代码。

2、Pre-training NEZHA Models

2.1、Preliminaries:  BERT Model & Positional Encoding

BERT(来自 Transformer 的双向编码器表示)是一种预训练的语言模型，它是 Transformer 编码器的堆栈。每个 Transformer 编码器都是一个多头自注意力层，后跟一个位置前馈网络。它使用每个子层周围的残差连接，然后进行层归一化。关于 Transformer 架构的更多细节，我们请读者参考 [9]。 BERT 训练数据中的每个样本都是一对句子。在每个样本中，12% 的标记被屏蔽，1.5% 的标记被词汇表中的另一个标记随机替换。此外，在训练集中，每个样本（包含句子 A 和 B）的构造如下。 50% 的时候，B 实际上是 A 的下一个句子，而 50% 的时候 B 是语料库中的一个随机句子，不是 A 的下一个句子。在预训练阶段，BERT 有两个任务。一种是掩码语言建模（MLM），旨在从其他令牌中预测掩码令牌。第二个预训练任务是下一句预测（NSP）。它预测每个训练样本中的第二个句子是否是第一个句子的下一个句子。从某种意义上说，BERT 可以被视为一种去噪自动编码器，因为它的训练目标之一是恢复添加了噪声的数据。

2.2、Functional Relative Position Encoding

在当前版本的NEZHA中，我们采用了功能性相对位置编码，其中输出和注意分数的计算涉及到它们的相对位置的正弦函数。这个想法是受Transformer[9]中采用的函数式绝对位置编码的启发。具体来说，在我们的模型中，$a_{i j}^{V}$和$a_{i j}^{K}$都来自正弦函数，并在模型训练期间固定下来。在本技术报告的其余部分，为了清楚起见，我们用$a_{i j}$来表示$a_{i j}^{V}$和$a_{i j}^{K}$的表述。分别考虑$a_{i j}$的维度$2 \cdot k$和维度$2 \cdot k+1$。
$$
\begin{gathered}
a_{i j}[2 k]=\sin \left((j-i) /\left(10000^{\frac{2 \cdot k}{d z}}\right)\right) \\
a_{i j}[2 k+1]=\cos \left((j-i) /\left(10000^{\frac{2 \cdot k}{d z}}\right)\right)
\end{gathered}
$$
也就是说，位置编码的每个维度都对应着一个正弦波，不同维度的正弦波函数具有不同的波长。在上述公式中，dz等于NEZHA模型每头的隐藏尺寸（即隐藏尺寸除以头数）。波长形成一个从2π到10000-2π的几何级数。我们选择固定的正弦函数主要是因为它可能允许模型推断出比训练期间遇到的序列长度更长的序列。

2.3、Whole Word Masking

在 vanilla BERT 中，每个标记或汉字被随机掩码。在 [8] 中，发现整个词掩码 (WWM) 策略比随机掩码更有效地训练 BERT。在WWM中，一旦一个汉字被掩码，属于同一个汉字的其他字符都被一起掩码。在为 NEZHA 实现 WWM 时，我们使用了分词工具 Jieba2 来进行中文分词（即找到中文单词的边界）。在 WWM 训练数据中，每个样本包含若干个被掩码的汉字，被掩码的汉字总数约为其长度的 12% 和 1.5% 的随机替换字符。

2.4、Mixed Precision Training

在我们的 Nezha 模型的预训练中，我们采用了混合精度训练的技术 [13]。该技术可以将训练速度提高 2-3 倍，还可以减少模型的空间消耗，因此可以利用更大的批量大小。

传统上，深度神经网络的训练使用FP32（即单精度浮点格式）来呈现训练中涉及的所有变量（包括模型参数和梯度）。混合精度训练在训练中采用混合精度。具体来说，它在模型中保持一个单精度的权重副本（称为主权重）。在每个训练迭代中，它将主权重四舍五入为FP16（即半精度浮点格式），并对以FP16格式存储的权重、激活和梯度进行前向和后向传递。最后，它将梯度转换为FP32格式，并通过使用FP32梯度更新主权重。

2.5、LAMB Optimizer

LAMB优化器[14]是为深度神经元网络的大批量同步分布式训练而设计的。用大批量的迷你训练DNN是一种有效的方法，可以加快训练速度。然而，如果不仔细调整学习率的时间表，当批量大小超过一定的阈值时，性能就会受到很大的损害。LAMB优化器不需要手工调整学习率，而是采用了一种通用的适应策略，同时通过理论分析提供对收敛的洞察力。该优化器通过使用非常大的批处理量（在[14]中高达30k以上）来加快BERT的训练，而不会产生性能损失，甚至在许多任务中获得最先进的性能。值得注意的是，BERT的训练时间从3天大幅减少到76分钟。

3、Experiments

在本节中，我们报告了针对中文文本预训练我们的NEZHA模型的实验结果，以及对中文NLU下游任务的微调。应该指出的是，这些训练技术并不局限于中文，也可以随时应用于其他语言。

3.1、Experimental Setting

Datasets:  我们采用三个中文语料库对NEZHA模型进行预训练。

- Chinese Wikipedia.  中文维基百科是一部包含 1,067,552 篇文章的中文百科全书。我们下载了最新的中文维基百科转储，并使用名为 WikiExtractor4 的工具清理了原始数据。清理后的语料库包含简体中文和繁体中文，大约有 2.02 亿个标记。
- Baidu Baike. 我们从百度百科爬取网页，百度百科是中国搜索引擎百度拥有和制作的中文、协作、基于网络的百科全书。截至2018年8月，百度百科拥有超过1540万篇文章。清理后的语料库包含 4,734M 个标记。
- Chinese News

对于上述每个语料库，我们为NEZHA准备了两个版本的预训练数据。第一个版本的处理方式与文献[1]中的相同，其中包含12%的屏蔽汉字和1.5%的随机替换汉字。我们使用BERT Github项目6提供的工具，将文本数据转换为预训练的例子。第二个版本是基于全词屏蔽（WWM）策略。我们用中文分词器Jieba创建了WWM预训练例子，用于识别中文单词的边界。在WWM例子中，每个样本都包含几个被屏蔽的中文词，被屏蔽的汉字总数大约占其长度的12%，还有1.5%的随机替换字符。

4、Conclusion

在技术报告中，我们介绍了我们在中文语料库上训练大规模预训练语言模型NEZHA的实践。我们采用了一种有效的功能性相对位置编码方案，与其他位置编码相比，它导致了无表的改进。NEZHA模型的预训练还整合了一些技术，包括全词屏蔽策略、混合精度训练和LAMB优化器。实验表明，我们的模型可以在几个中文自然语言理解任务上取得最先进的性能。在未来，我们计划继续在中文和其他语言上改进NEZHA，并将NEZHA的应用扩展到更多的场景。



## Unified Language Model Pre-training for Natural Language Understanding and Generation

- 用于自然语言理解和生成的统一语言模型预训练

本文提出了一种新的统一预训练语言模型（UNILM），可以针对自然语言理解和生成任务进行微调。该模型使用三种类型的语言建模任务进行预训练：单向、双向和序列到序列预测。统一建模是通过使用共享的 Transformer 网络并利用特定的自注意力掩码来控制预测条件的上下文来实现的。 UNILM 在 GLUE 基准测试以及 SQuAD 2.0 和 CoQA 问答任务上与 BERT 相比毫不逊色。此外，UNILM 在五个自然语言生成数据集上取得了新的最先进的结果，包括将 CNN/DailyMail 抽象摘要 ROUGE-L 提高到 40.51（2.04 绝对提高），将 Gigaword 抽象摘要 ROUGE-L 提高到 35.75（0.86绝对改进），CoQA 生成问答 F1 得分为 82.5（37.1 绝对改进），SQuAD 问题生成 BLEU-4 为 22.12（3.75 绝对改进），DSTC7 基于文档的对话响应生成 NIST-4 为 2.67（人类性能为 2.65）。

1、介绍

语言模型 (LM) 预训练在各种自然语言处理任务中显着提升了现有技术 [8, 29, 19, 31, 9, 1]。预训练的 LM 通过使用大量文本数据根据上下文预测单词来学习上下文文本表示，并且可以进行微调以适应下游任务。

不同的预测任务和训练目标被用于预训练不同类型的LM，如表1所示。ELMo[29]学习两个单向的LM：一个正向LM从左到右读取文本，一个反向LM从右到左编码文本。GPT[31]使用一个从左到右的转换器[43]来逐字预测文本序列。相比之下，BERT[9]采用了一个双向Transformer编码器来融合左右两边的语境来预测被屏蔽的单词。虽然BERT极大地提高了各种自然语言理解任务的性能[9]，但其双向性的特点使其难以应用于自然语言生成任务[44]。

在这项工作中，我们提出了一个新的统一的预训练语言模型（UNILM），可以应用于自然语言理解（NLU）和自然语言生成（NLG）任务。UNILM是一个多层的Transformer网络，在大量的文本上联合预训练，为三种类型的无监督语言建模目标进行优化，如表2所示。特别是，我们设计了一套cloze(填空)任务[42]，其中根据上下文预测一个被掩盖的单词。这些cloze任务的不同之处在于如何定义上下文。对于从左到右的单向LM，要预测的遮蔽词的上下文包括其左边的所有词。对于从右到左的单向LM，其上下文由右边的所有词组成。对于双向LM，上下文由左右两边的词组成[9]。对于序列到序列的LM，第二个（目标）序列中要预测的单词的上下文由第一个（源）序列中的所有单词和目标序列中其左边的单词组成。

与BERT类似，预训练的UNILM可以进行微调（必要时增加特定任务层）以适应各种下游任务。但是，与主要用于NLU任务的BERT不同，UNILM可以使用不同的自我注意掩码（第2节）进行配置，为不同类型的语言模型聚合上下文，因此可以同时用于NLU和NLG任务。

拟议的UNILM有三个主要优点。首先，统一的预训练程序导致了一个单一的Transformer LM，它为不同类型的LM使用共享的参数和架构，减轻了单独训练和托管多个LM的需要。第二，参数共享使学习到的文本表征更加通用，因为它们是针对不同的语言建模目标共同优化的，其中语境的利用方式不同，减轻了对任何单一LM任务的过度拟合。第三，除了应用于NLU任务外，UNILM作为一个序列到序列的LM（第2.3节），使其成为NLG的自然选择，如抽象化的总结和问题生成。

实验结果表明，我们的模型作为一个双向编码器，在GLUE基准和两个抽取式问题回答任务（即SQuAD 2.0和CoQA）上与BERT相比有优势。此外，我们还证明了UNILM在五个NLG数据集上的有效性，它被用作序列到序列的模型，在CNN/DailyMail和Gigaword的抽象总结、SQuAD问题生成、CoQA生成性问题回答和DSTC7对话回应生成上创造了新的最先进的结果。



2、Unified Language Model Pre-training

给定一个输入序列 $x=x_{1} \cdots x_{|x|}$，UNILM 为每个标记获得一个上下文化的向量表示。 如图 1 所示，预训练针对几个无监督语言建模目标（即单向 LM、双向 LM 和序列到序列 LM）优化了共享 Transformer [43] 网络。 为了控制对要预测的词标记的上下文的访问，我们使用不同的掩码进行自我注意。 换句话说，我们使用掩码来控制令牌在计算其上下文表示时应该关注多少上下文。 一旦 UNILM 进行了预训练，我们就可以使用针对下游任务的任务特定数据对其进行微调。

<img src="../imgs/UniLM_Figure-1.png" alt="UniLM_Figure-1" style="zoom:67%;" />

<center>Figure 1: 统一LM预训练概述。模型参数在不同的LM目标（即双向LM、单向LM和序列到序列LM）中共享。我们使用不同的自我注意掩码来控制每个单词标记对上下文的访问。从右到左的LM与从左到右的LM类似，为了简洁起见，图中省略了这一选项。 </center>

2.1、Input Representation

输入x是一个单词序列，对于单向LM来说，它是一个文本片段，对于双向LM和序列到序列的LM来说，它是一对打包在一起的片段。我们总是在输入的开头添加一个特殊的序列开始（[SOS]）标记，并在每个片段的结尾添加一个特殊的序列结束（[EOS]）标记。[EOS]不仅标志着NLU任务中的句子边界，而且还用于模型学习何时终止NLG任务中的解码过程。输入表示法与BERT[9]相同。通过WordPiece[48]将文本标记为子词单元。对于每个输入标记，它的向量表示是由相应的标记嵌入、位置嵌入和段嵌入相加而计算出来的。由于UNILM是使用多个LM任务进行训练的，段嵌入也起到了LM识别器的作用，我们为不同的LM目标使用不同的段嵌入。

2.2、Backbone Network:  Multi-Layer Transformer

输入向量 $\left\{\mathbf{x}_{i}\right\}_{i=1}^{|x|}$ 首先被打包到$\mathbf{H}^{0}=\left[\mathbf{x}_{1}, \cdots, \mathbf{x}_{|x|}\right]$中，然后用L层的Transformer $\mathbf{H}^{l}=\operatorname{Transformer}_{l}\left(\mathbf{H}^{l-1}\right), l \in[1, L]$编码为不同层次的抽象的上下文表示。在每个Transformer块中，多个自注意头被用来聚合前一层的输出向量。对于第l个变形器层，自注意头Al的输出是通过以下方式计算的：
$$
\begin{aligned}
\mathbf{Q} &=\mathbf{H}^{l-1} \mathbf{W}_{l}^{Q}, \quad \mathbf{K}=\mathbf{H}^{l-1} \mathbf{W}_{l}^{K}, \quad \mathbf{V}=\mathbf{H}^{l-1} \mathbf{W}_{l}^{V} \\
\mathbf{M}_{i j} &= \begin{cases}0, & \text { allow to attend } \\
-\infty, & \text { prevent from attending }\end{cases} \\
\mathbf{A}_{l} &=\operatorname{softmax}\left(\frac{\mathbf{Q } \mathbf{K}^{\top}}{\sqrt{d_{k}}}+\mathbf{M}\right) \mathbf{V}_{l}
\end{aligned}
$$
其中，前一层的输出$\mathbf{H}^{l-1} \in \mathbb{R}^{|x| \times d_{h}}$分别用参数矩阵 $\mathbf{W}_{l}^{Q}, \mathbf{W}_{l}^{K}, \mathbf{W}_{l}^{V} \in \mathbb{R}^{d_{h} \times d_{k}}$线性投影到查询、键和值的三要素上，屏蔽矩阵$\mathbf{M} \in \mathbb{R}^{|x| \times|x|}$决定一对令牌是否可以相互关注。

我们使用不同的掩码矩阵M来控制一个标记在计算其语境化表示时可以关注的语境，如图1所示。以双向的LM为例。掩码矩阵的元素都是0，表明所有的标记都可以相互访问。

2.3、Pre-training Objectives

我们使用为不同语言建模目标设计的四个完形填空任务对 UNILM 进行预训练。在完形填空任务中，我们在输入中随机选择一些 WordPiece 标记，并用特殊标记 [MASK] 替换它们。然后，我们将它们对应的由 Transformer 网络计算的输出向量输入到 softmax 分类器中，以预测掩码标记。学习 UNILM 的参数以最小化使用预测标记和原始标记计算的交叉熵损失。值得注意的是，使用完形填空任务可以对所有 LM 使用相同的训练过程，单向和双向。

<span style='color:brown'>**单向LM**</span>： 我们同时使用从左到右和从右到左的LM目标。以从左到右的LM为例。每个令牌的表示只编码向左的上下文令牌和它本身。例如，为了预测 "x1x2 [MASK] x4 "的遮蔽标记，只能使用标记x1,x2和其本身。这是通过使用三角矩阵的自我注意掩码M（如公式（2））来实现的，其中自我注意掩码的上三角部分被设置为-∞，其他元素为0，如图1所示。同样，一个从右到左的LM预测一个标记的条件是它的未来（右边）环境。

<span style='color:brown'>**双向LM**</span>：在 [9] 之后，双向 LM 允许所有令牌在预测中相互关注。它从两个方向对上下文信息进行编码，并且可以生成比其单向对应物更好的文本上下文表示。如等式（2）所示，self-attention mask M 是一个零矩阵，因此每个标记都可以参与输入序列中的所有位置。

<span style='color:brown'>**序列到序列 LM**</span>：如图1所示，对于预测来说，第一个（源）段的标记可以从段内的两个方向相互关注，而第二个（目标）段的标记只能关注目标段内向左的上下文和自身，以及源段内的所有标记。例如，给定源语段t1t2和其目标语段t3t4t5，我们将输入"[SOS] t1 t2 [EOS] t3 t4 t5 [EOS]"输入模型。虽然t1和t2都可以访问前四个标记，包括[SOS]和[EOS]，但t4只能注意到前六个标记。

图 1 显示了用于序列到序列 LM 目标的自注意掩码 M。 M 的左边部分设置为 0，以便所有令牌都可以参加第一段。右上部分设置为-∞以阻止从源段到目标段的注意力。此外，对于右下部分，我们将其上三角部分设置为 -∞，将其他元素设置为 0，这可以防止目标片段中的标记出现在它们未来（右）的位置。

在训练期间，我们在两个段中随机选择标记，并用特殊标记 [MASK] 替换它们。 该模型被学习以恢复被屏蔽的标记。 由于这对源文本和目标文本在训练中被打包为一个连续的输入文本序列，我们隐含地鼓励模型学习两个段之间的关系。 为了更好地预测目标段中的标记，UNILM 学习有效地对源段进行编码。 因此，为序列到序列 LM 设计的完形填空任务，也称为编码器-解码器模型，同时预训练了一个双向编码器和一个单向解码器。 用作编码器-解码器模型的预训练模型可以轻松适应广泛的条件文本生成任务，例如抽象摘要。

<span style='color:brown'>**Next Sentence Prediction**</span>：

对于双向的LM，我们还包括下一句预测任务，用于预训练，如[9]。

2.4、Pre-training Setup

总体训练目标是上述不同类型 LM 目标的总和。具体来说，在一个训练批次中，1/3 的时间我们使用双向 LM 目标，1/3 的时间我们使用序列到序列的 LM 目标，以及从左到右和从右到-left LM 目标以 1/6 的速率进行采样。为了公平比较，UNILM 的模型架构遵循 BERTLARGE [9] 的模型架构。凝胶激活 [18] 用作 GPT [31]。具体来说，我们使用具有 1,024 个隐藏大小和 16 个注意力头的 24 层 Transformer，其中包含大约 340M 参数。 softmax 分类器的权重矩阵与令牌嵌入相关联。 UNILM 由 BERTLARGE 初始化，然后使用英语 Wikipedia2 和 BookCorpus [53] 进行预训练，其处理方式与 [9] 相同。词汇量为 28,996。输入序列的最大长度为 512。令牌掩蔽概率为 15%。在掩码位置中，80% 的时间我们用 [MASK] 替换标记，10% 的时间用随机标记替换，其余的保留原始标记。此外，80% 的时间我们每次都随机屏蔽一个标记，20% 的时间我们屏蔽一个二元组或三元组。

2.5、Fine-tuning on Downstream NLU and ULG Tasks

对于NLU任务，我们将UNILM微调为一个双向的Transformer编码器，就像BERT。以文本分类为例。我们使用[SOS]的编码向量作为输入的表示，表示为$\mathbf{h}_{1}^{L}$，并将其送入随机初始化的softmax分类器（即特定任务的输出层），其中类别概率计算为$\operatorname{softmax}\left(\mathbf{h}_{1}^{L} \mathbf{W}^{C}\right)$，其中$\mathbf{W}^{C} \in \mathbb{R}^{d_{h} \times C}$是一个参数矩阵，C是类别的数量。我们通过更新预训练的LM和添加的softmax分类器的参数来最大化标记的训练数据的可能性。

对于 NLG 任务，我们以序列到序列的任务为例。微调过程类似于第 2.3 节中使用自注意掩码的预训练。让 S1 和 S2 分别表示源序列和目标序列。我们将它们与特殊令牌打包在一起，形成输入“[SOS] S1 [EOS] S2 [EOS]”。该模型通过随机屏蔽目标序列中一定百分比的标记进行微调，并学习恢复被屏蔽的单词。训练目标是最大化给定上下文的掩码标记的可能性。值得注意的是，标记目标序列结束的[EOS]也可以在微调期间被屏蔽，因此当这种情况发生时，模型会学习何时发出[EOS]以终止目标序列的生成过程.

3、Experiments

我们已经对 NLU（即 GLUE 基准测试和抽取式问答）和 NLG 任务（即抽象摘要、问题生成、生成式问答和对话响应生成）进行了实验。



4、Conclution and Future Work

我们提出了一个统一的预训练模型--UNILM，该模型通过共享参数对几个LM目标进行联合优化。双向、单向和序列对序列的LM的统一使我们能够直接对预训练的UNILM进行微调，以完成NLU和NLG任务。实验结果表明，在GLUE基准和两个问题回答数据集上，我们的模型与BERT相比有优势。此外，UNILM在五个NLG数据集上的表现优于之前的最先进的模型。CNN/DailyMail和Gigaword的抽象总结、SQuAD问题生成、CoQA生成性问题回答和DSTC7对话响应生成。

可以从以下几个方面推进工作：

- 我们将通过在网络规模的文本语料库上训练更多的时期和更大的模型来突破当前方法的极限。同时，我们还将在终端应用和消融实验上进行更多的实验，以研究模型能力以及在同一网络上预训练多个语言建模任务的好处;
- 在我们当前的实验中，我们专注于单语 NLP 任务。我们也有兴趣扩展 UNILM 以支持跨语言任务 [6]。
- 我们将对 NLU 和 NLG 任务进行多任务微调，这是多任务深度神经网络 (MT-DNN) [26] 的自然扩展。
