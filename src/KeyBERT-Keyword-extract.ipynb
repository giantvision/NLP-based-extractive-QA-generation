{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KeyBERT-Keyword-extract.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1z-mO84e90JbstHn_lFop7Hordtgu5N_s","authorship_tag":"ABX9TyOzNagzPcDlrH6KNN/KdVnA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"e4GYL7ChQ6Bd"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","import jieba\n","\n","def tokenize_zh(text):\n","    words = jieba.lcut(text)\n","    return words\n","\n","vectorizer = CountVectorizer(tokenizer=tokenize_zh)"]},{"cell_type":"code","source":["pip install keybert"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bmQD9cYwR4Wh","executionInfo":{"status":"ok","timestamp":1643182020562,"user_tz":-480,"elapsed":16304,"user":{"displayName":"Aspirin 007","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11941172714059383624"}},"outputId":"f9a6adf3-d586-47c7-ca57-0ee377ee5d52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keybert\n","  Downloading keybert-0.5.0.tar.gz (19 kB)\n","Collecting sentence-transformers>=0.3.8\n","  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 3.1 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from keybert) (1.0.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from keybert) (1.19.5)\n","Collecting rich>=10.4.0\n","  Downloading rich-11.0.0-py3-none-any.whl (215 kB)\n","\u001b[K     |████████████████████████████████| 215 kB 12.3 MB/s \n","\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.4.0->keybert) (2.6.1)\n","Collecting commonmark<0.10.0,>=0.9.0\n","  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n","\u001b[K     |████████████████████████████████| 51 kB 3.3 MB/s \n","\u001b[?25hCollecting colorama<0.5.0,>=0.4.0\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: typing-extensions<5.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from rich>=10.4.0->keybert) (3.10.0.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (3.0.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.1)\n","Collecting transformers<5.0.0,>=4.6.0\n","  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n","\u001b[K     |████████████████████████████████| 3.4 MB 59.6 MB/s \n","\u001b[?25hCollecting tokenizers>=0.10.3\n","  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n","\u001b[K     |████████████████████████████████| 6.8 MB 25.7 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.62.3)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (1.10.0+cu111)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.11.1+cu111)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.2.5)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 47.2 MB/s \n","\u001b[?25hCollecting huggingface-hub\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 4.2 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 48.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.23.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 48.1 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.4.2)\n","Collecting tokenizers>=0.10.3\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 40.4 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (4.10.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2019.12.20)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.7.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (7.1.2)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (7.1.2)\n","Building wheels for collected packages: keybert, sentence-transformers\n","  Building wheel for keybert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keybert: filename=keybert-0.5.0-py3-none-any.whl size=20491 sha256=c433b86b7dc37ce664fa824b44b6f1a5f640db537f38fc215cb00eed7f9b9ccd\n","  Stored in directory: /root/.cache/pip/wheels/99/1f/3f/590d2997adbb2d0e1f82e8ee05d42d6910e92c3ed283015ff8\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=120999 sha256=d9c2cacba46e37c0a5ce9499ee0374d66d42e3553ee5ea3154d171a8dfa6ea2d\n","  Stored in directory: /root/.cache/pip/wheels/90/f0/bb/ed1add84da70092ea526466eadc2bfb197c4bcb8d4fa5f7bad\n","Successfully built keybert sentence-transformers\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, commonmark, colorama, sentence-transformers, rich, keybert\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed colorama-0.4.4 commonmark-0.9.1 huggingface-hub-0.4.0 keybert-0.5.0 pyyaml-6.0 rich-11.0.0 sacremoses-0.0.47 sentence-transformers-2.1.0 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.15.0\n"]}]},{"cell_type":"code","source":["from keybert import KeyBERT\n","\n","kw_model = KeyBERT()\n","\n","doc = \"我们都是中国人，中国人不骗中国人。\"\n","keywords = kw_model.extract_keywords(doc, vectorizer=vectorizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iylzh8aVRwc7","executionInfo":{"status":"ok","timestamp":1643182084348,"user_tz":-480,"elapsed":3563,"user":{"displayName":"Aspirin 007","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11941172714059383624"}},"outputId":"6a7f4960-7c2e-4ece-c40e-cfbd4b2b85bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  \"The parameter 'token_pattern' will not be used\"\n","Building prefix dict from the default dictionary ...\n","Dumping model to file cache /tmp/jieba.cache\n","Loading model cost 1.201 seconds.\n","Prefix dict has been built successfully.\n"]}]},{"cell_type":"code","source":["print(keywords)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g783RI58SOdS","executionInfo":{"status":"ok","timestamp":1643182102287,"user_tz":-480,"elapsed":503,"user":{"displayName":"Aspirin 007","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11941172714059383624"}},"outputId":"2298807e-3898-47f0-adfe-ff4683c95caa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('中国', 0.6228), ('人不骗', 0.568), ('我们', 0.5606), ('都', 0.4814), ('人', 0.3894)]\n"]}]},{"cell_type":"code","source":["text = '电子游戏产业最新的做法是，游戏玩家的挖掘成为游戏开发的内容。游戏玩家的挖掘成为了游戏内容的第四方开发者的焦点，使得更多开放源代码模型的游戏设计，开发和工程出现了。玩家创建用户修改的游戏（MOD），在某些情况下与原游戏一样流行，甚至比原游戏更受欢迎。这方面的一个例子是游戏《反恐精英》，开始是电子游戏《半条命》一个的模组（MOD）并最终成为了一个非常成功的发行游戏。虽然这种“修改者共享”可能只增加特定的游戏用户群的约1%，而数量的增加将提供更多的修改游戏机会（例如发放源代码），并伴随国际玩家群体的上升而增加。据Ben Sawyer推测，到2012年将存在多达600,000在线游戏公共开发者。这将有效地为游戏产业价值链增加一个新的组成部分，并且如果继续走向成熟，它将融入整个行业。'\n","\n","keywords = kw_model.extract_keywords(text, vectorizer=vectorizer)\n","print(keywords)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fI7UwBR4TM3h","executionInfo":{"status":"ok","timestamp":1643182394494,"user_tz":-480,"elapsed":1050,"user":{"displayName":"Aspirin 007","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11941172714059383624"}},"outputId":"68532134-9ecc-4e6d-b10b-6a1f7a560694"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  \"The parameter 'token_pattern' will not be used\"\n"]},{"output_type":"stream","name":"stdout","text":["[('电子游戏', 0.3741), ('例子', 0.3463), ('增加', 0.2624), ('组成部分', 0.2372), ('发行', 0.2267)]\n"]}]},{"cell_type":"code","source":["text1 = '中国传统绘画，又称丹青，在中国又称国画，泛指中华文化的传统绘画艺术，是琴棋书画四艺之一。狭义的国画指青绿设色画水墨画，而广义的国画则是中国传统风格的壁画、锦画、刺绣、重彩、水墨画、石刻乃至年画和陶瓷上的绘画等的艺术，也包括近代的中国油画和水彩画等。 国画历史悠久，在东周墓葬中出土过最早的帛画作《龙凤仕女图》，传世最早最完整的作品是顾恺之《女史箴图》的唐朝摹本。在五代十国以后中国文人艺术家得到了很高的社会地位。宋代以前绘图在绢帛上，材料昂贵，因此国画题材多以王宫贵族肖像或生活记录等，直至宋元两代后，纸材改良，推广与士大夫文人画兴起等，让国画题材技法多元。明代绘画推广到大众，成为市民生活的一部分，风俗画因此产生。清末，绘画材料多元，朝多方面发展。'\n","\n","keywords = kw_model.extract_keywords(text1, vectorizer=vectorizer)\n","print(keywords)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wjZy7Xl6Trbb","executionInfo":{"status":"ok","timestamp":1643182528134,"user_tz":-480,"elapsed":730,"user":{"displayName":"Aspirin 007","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11941172714059383624"}},"outputId":"6a54a877-2d35-4b0e-e7a0-bcc3c1c42cf8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  \"The parameter 'token_pattern' will not be used\"\n"]},{"output_type":"stream","name":"stdout","text":["[('国画', 0.3988), ('市民', 0.3691), ('中国', 0.3673), ('历史悠久', 0.3604), ('五代十国', 0.3496)]\n"]}]},{"cell_type":"code","source":["text2 = '《女史箴图》长卷，传顾恺之的作品，现剩唐绢临本藏于大英博物馆，原为清宫藏画，在英法联军火烧圆明园时被抢劫到英国。顾恺之多才，工诗赋，善书法，被时人称为“才绝、画绝、痴绝”，他的画线条连绵流畅，如“春蚕吐丝”。著有《论画》、《魏晋胜流画赞 (摹拓妙法)》和《画云台山记》三本绘画理论书籍(以上三篇文章现今存在最早版本由唐张彦远历代名画记抄录以传)，提出“以形写神”、“尽在阿堵中”的传神理论。其陆探微、张僧繇合称“六朝三大家”。'\n","\n","keywords = kw_model.extract_keywords(text2, vectorizer=vectorizer)\n","print(keywords)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0lz2JBcyUv55","executionInfo":{"status":"ok","timestamp":1643182810233,"user_tz":-480,"elapsed":544,"user":{"displayName":"Aspirin 007","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11941172714059383624"}},"outputId":"18108d0e-beaf-4923-d95e-d0d111d1424f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  \"The parameter 'token_pattern' will not be used\"\n"]},{"output_type":"stream","name":"stdout","text":["[('顾恺之', 0.418), ('传神', 0.3886), ('三篇', 0.3873), ('春蚕', 0.387), ('三本', 0.3751)]\n"]}]},{"cell_type":"code","source":["text3 = '太极拳是中国传统武术，与形意拳和八卦掌并称中国三大内家拳。太极拳讲究中定、放松、心静、慢练及九曲珠，和外家拳明显不一样。据中国国家体委科研课题考证，太极拳创始者是元明之交13世纪的武当道士张三丰。吴图南《太极拳之研究》中也考证了张三丰对太极拳的贡献。'\n","\n","keywords = kw_model.extract_keywords(text3, vectorizer=vectorizer)\n","print(keywords)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MD9Qcx_jVDwx","executionInfo":{"status":"ok","timestamp":1643182874757,"user_tz":-480,"elapsed":522,"user":{"displayName":"Aspirin 007","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11941172714059383624"}},"outputId":"8f0ea2e6-caac-4b97-b30f-0f3b24f92371"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  \"The parameter 'token_pattern' will not be used\"\n"]},{"output_type":"stream","name":"stdout","text":["[('传统武术', 0.5789), ('太极拳', 0.5357), ('武当', 0.5172), ('明之交', 0.5018), ('国家体委', 0.4833)]\n"]}]},{"cell_type":"code","source":["text4 = '临沂市，简称沂，古称琅琊、沂州，是中华人民共和国山东省下辖的地级市，位于山东省东南部，因临沂河而得名。市境西接枣庄市、济宁市，北临泰安市、淄博市、潍坊市，东界日照市，东南达江苏省连云港市，南毗江苏省徐州市。地处山东丘陵沂蒙山区与沂沭河冲积平原过渡带，地势北高南低。沂山、蒙山、尼山三大山脉分布于北部，中部为丘陵，南部为平原。沂沭泗河水系呈脉状分布，沂河、沭河、泗水皆发源于沂蒙山流入东海，还有东汶河、蒙河、祊河等河流。全市总面积17,191平方公里，人口1101.84万，是山东省面积最大、人口最多的地级市，市人民政府驻兰山区。'\n","\n","keywords = kw_model.extract_keywords(text4, vectorizer=vectorizer)\n","print(keywords)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kkVzXmVGVc05","executionInfo":{"status":"ok","timestamp":1643182980850,"user_tz":-480,"elapsed":769,"user":{"displayName":"Aspirin 007","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11941172714059383624"}},"outputId":"9e503ca8-b738-4e7b-b51b-066cc550834d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  \"The parameter 'token_pattern' will not be used\"\n"]},{"output_type":"stream","name":"stdout","text":["[('东南部', 0.4526), ('南部', 0.4455), ('沂蒙山区', 0.4333), ('兰山区', 0.4296), ('北高南', 0.4284)]\n"]}]},{"cell_type":"code","source":["text5 = '《史记》，最早称为《太史公书》或《太史公记》，是西汉汉武帝时期的任职太史令的司马迁（太史公）编写的纪传体史书，记载自传说中的黄帝至汉武帝太初年间共二千五百年的中国历史，与后来的《汉书》、《后汉书》、《三国志》合称“前四史”。'\n","\n","keywords = kw_model.extract_keywords(text5, vectorizer=vectorizer)\n","print(keywords)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8DApFwpLWPka","executionInfo":{"status":"ok","timestamp":1643183209512,"user_tz":-480,"elapsed":1331,"user":{"displayName":"Aspirin 007","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11941172714059383624"}},"outputId":"ed1650b9-d51a-4363-a2d9-3a24e093dc61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  \"The parameter 'token_pattern' will not be used\"\n"]},{"output_type":"stream","name":"stdout","text":["[('四史', 0.6052), ('历史', 0.5864), ('汉武帝', 0.579), ('太史公', 0.5633), ('太史令', 0.5575)]\n"]}]},{"cell_type":"markdown","source":["## 如何在Python和Jieba中提高中文文本分词的性能"],"metadata":{"id":"zzVT7fY4ghE2"}},{"cell_type":"markdown","source":["通过阅读这篇文章，你将学会向Jieba添加自己的习惯词，以提高标记化的性能。在处理特定领域的自然语言处理（NLP）任务时，必须控制标记化过程，因为有相当多的自定义词被定义为名词。此外，由于中文不是一种空间标记的语言，它的情况会更糟糕。可能会出现最终的结果与你预期的不一样的情况。让我们看一下下面的例子。"],"metadata":{"id":"2FyxcWd6ieF4"}},{"cell_type":"code","source":["import jieba.posseg as pseg\n","import jieba\n","\n","text = \"于吉大招叫什么。于吉怒气技叫什么\"\n","\n","words = pseg.cut(text)\n","for w in words:\n","  print('%s %s' % (w.word, w.flag))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XPRbZH0WeUCB","executionInfo":{"status":"ok","timestamp":1643185512406,"user_tz":-480,"elapsed":1042,"user":{"displayName":"Aspirin 007","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11941172714059383624"}},"outputId":"d3b1dc84-b98b-4f0e-fd59-5114cdcd5561"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["于 p\n","吉大招 nr\n","叫 v\n","什么 r\n","。 x\n","于吉 nr\n","怒气 n\n","技叫 n\n","什么 r\n"]}]},{"cell_type":"markdown","source":["从一个正常人的角度来看，这句话应该被分词如下：\n","\n","['于吉', '大招', '叫', '什么']；\n","\n","然而，当你用Jieba对其进行标记时，情况就不是这样了，因为你会得到以下结果：\n","\n","['于', '吉大招', '叫', '什么']\n","\n","看看如何解决这个问题。"],"metadata":{"id":"RC2lTcG-jKPh"}},{"cell_type":"markdown","source":["从上面的图片中，我们可以看到于吉这个词在第二条语句中被正确标记了，但在第一条语句中却没有。我们将使用Jieba模块提供的add_word函数，把它作为一个名词添加到现有的词典中。事实上，Jieba还有一个叫做 suggest_freq 的函数。\n","\n","- add_word(word, freq=None, tag=None) — 在程序中动态修改字典；\n","- suggest_freq(segment, tune=True) - 调整单个单词的频率，使其可以（或不能）被分割。"],"metadata":{"id":"I9hqlObuk9tQ"}},{"cell_type":"markdown","source":["让我们通过在其下方添加以下代码来尝试一下。标签 nr 代表词性标注下的人名。"],"metadata":{"id":"TDmCA9JElSgI"}},{"cell_type":"code","source":["jieba.add_word('于吉', freq=None, tag='nr')"],"metadata":{"id":"jQhJ5tJsfpI5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"于吉大招叫什么。于吉怒气技叫什么\"\n","\n","words = pseg.cut(text)\n","for w in words:\n","  print('%s %s' % (w.word, w.flag))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_djFQEO4f75A","executionInfo":{"status":"ok","timestamp":1643185688437,"user_tz":-480,"elapsed":521,"user":{"displayName":"Aspirin 007","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11941172714059383624"}},"outputId":"cbfa8058-e5fe-469c-901e-22f4e40fae9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["于吉 nr\n","大 a\n","招叫 v\n","什么 r\n","。 x\n","于吉 nr\n","怒气 n\n","技叫 n\n","什么 r\n"]}]},{"cell_type":"markdown","source":["很明显，这次Jieba正确地标记了于吉这个词。然而，我们仍然有一个问题，即大被当作形容词来处理。大招应该是一个独立的名词。让我们用下面的代码在词典中添加另一个词"],"metadata":{"id":"SL3xi0dGleTv"}},{"cell_type":"code","source":["jieba.add_word('大招', freq=None, tag='n')\n","\n","text = \"于吉大招叫什么。于吉怒气技叫什么\"\n","\n","words = pseg.cut(text)\n","for w in words:\n","  print('%s %s' % (w.word, w.flag))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dtximULif_JZ","executionInfo":{"status":"ok","timestamp":1643185786298,"user_tz":-480,"elapsed":508,"user":{"displayName":"Aspirin 007","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11941172714059383624"}},"outputId":"3f9804b3-5f41-477a-840f-dd2e3cb5f6a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["于吉 nr\n","大招 n\n","叫 v\n","什么 r\n","。 x\n","于吉 nr\n","怒气 n\n","技叫 n\n","什么 r\n"]}]},{"cell_type":"markdown","source":["## 文本文件中的自定义词列表\n","\n","话虽如此，在现实生活中的用例中，风俗词的列表要大得多，可能有数千个。因此，手动添加每个单词可能是一项艰巨的任务。强烈建议将其放入文本文件并动态加载。最好的方法是将文本文件中的所有单词合并到一个由换行符分隔的文件中，如下所示：\n","\n","星光绝世\n","\n","不死火鸟\n","\n","战盟红包\n","\n","世界等级\n","\n","情有独钟\n","\n","铜质勋章\n","\n","战盟宴会\n"],"metadata":{"id":"umPJ71OQlkcv"}},{"cell_type":"markdown","source":["然后，在标记化过程之前添加以下代码。相应地修改文件的名称。"],"metadata":{"id":"aBSMgTRQl34u"}},{"cell_type":"code","source":["with open('noun_list.txt', 'r', encoding='utf-8') as f:\n","  custom_noun = f.readlines()\n","  for noun in custom_noun:\n","    jieba.add_word(noun.replace('\\n', '', freq=None, tag='n'))"],"metadata":{"id":"HgPF9Kv4l6DW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 其他关键词提取技术尝试"],"metadata":{"id":"QADqmATO8sfJ"}},{"cell_type":"markdown","source":[""],"metadata":{"id":"i9LaTGnv8yBx"}}]}